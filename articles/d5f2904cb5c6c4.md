---
title: "自分専用のAIレコーダーを自作した"
emoji: "🎙️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["Python","WhisperX","macOS","Automator","ChatGPT","pyannote","huggingface"]
published: true
---

最近YouTubeでいろんなガジェット系の方が紹介している「Plaud Note」。
これがあれば、自分の忘れっぽい記憶を外部化できて、第二の私を作れるのでは!? と思い、購入を検討しました。

いざ調べてみると少し気になる点があったので、**「Plaud Note Pro のようなAIレコーダー体験を、自分のMac上で再現できないか？（できるだけ追加コストなしで）」** というテーマで、  
WhisperX（ローカル文字起こし）→ 出力（json/txt）→ ChatGPTで要約、という流れで、**音声処理はローカルで、要約だけChatGPTに任せる**構成にした備忘録です。

前提、私はこの手の技術に殆ど触れてこなかったので、自作するとどんなもんなんだろうという興味本位の実験でもあります。

---

## 🎯 目指したこと

市販のAIレコーダー（例：Plaud Note Pro）はたしかに優秀です。  
ただし「**月額課金＋クラウド依存**」という部分に少し抵抗がありました。  

- 自分の声や会話をクラウドに上げたくない  
- 毎月お金を払い続けるのは現実的でない  
- どうせなら、自分のMac上で完結させたい  

そう思い、**「自分専用のAIレコーダー」をローカルで構築する**という挑戦を始めました。

---

## 🧩 目標

最初に目指したのは「市販AIレコーダーの全部入り体験」をローカルで再現することでした。
ただ、途中で方針転換して **話者分離を捨て、要約に寄せた構成**が最終形になりました。

### v1（最初に狙った“全部入り”）

[録音デバイス] → (音声ファイル転送) → [Mac]
↓
[WhisperX]：文字起こし（＋タイムスタンプ）
↓
[Pyannote.audio]：話者分離（誰が話したか識別）
↓
[outputs/]：テキスト・字幕・構造化データ

### v2（完成形：話者分離なしで運用できた）

[録音デバイス] →（USBでMacに接続）→ [自動取り込みスクリプト]
↓
[WhisperX]：文字起こし（＋タイムスタンプ）
↓
[outputs/<日付>/]：txt / srt / json（WhisperXの出力）
↓
[ChatGPT]：jsonを貼り付けて要約・整理

---

## 🧠 なぜ「話者分離」もやりたかったのか

最初は、Plaud Note Proのように「誰が何を言ったか」まで自動で残したいと思っていました。  
家庭内の会話だと、あとから見返したときにこういう情報があると便利です。  
仕事の議事録と同じで、会話ってその場では「分かったつもり」になりやすい。  
でも翌日になると、

- どっちが何を言ったんだっけ？
- どこまで合意したんだっけ？
- 明日やるのは誰だっけ？

が普通に起きます。

- **決定事項が誰発信だったか**（例：明日の段取り、買うもの、子どもの予定）
- **確認したいニュアンス**（お願い／相談／合意など）
- 「これ誰のタスクだっけ？」が減る

なので最初は WhisperX の `--diarize` 機能と Pyannote.audio を使って、話者分離まで含めた“フル機能版”を狙いました。

ただ結論から言うと、家庭内の短い会話・早口・方言などの条件が重なると安定しづらく、
**「文字起こしはローカルで作る → 要約はChatGPTの推論に寄せる」** に寄せたほうが、
運用がシンプルで成果も出ました。

---

## 🖥️ 使用環境

- macOS Sonoma  
- MacBook Air M3  
- Python 3.11（Homebrew経由でインストール）  
- GPUアクセラレーション：Metal（`CT2_USE_METAL=1`）

## ⚙️ 構築手順

ここでは、まず「完成形（話者分離なし）」を先に書き、その後に参考として「話者分離に挑戦した時の手順」を残します。

### 1️⃣ Python仮想環境を作成

```bash
% /opt/homebrew/bin/python3.11 -m venv ~/venvs/whisperx
% source ~/venvs/whisperx/bin/activate
```
ターミナル上で`(whisperx)`と表示されればOKです。
（venvパスは自分の環境に合わせて変更してください）

### 2️⃣ WhisperX をインストール（最終構成）

```bash
% pip install git+https://github.com/m-bain/whisperX.git
```

WhisperXはOpenAI Whisperの拡張版で、時間情報付きの文字起こしを行えます。
長時間の録音を想定していたので、タイムスタンプが残るのはかなり便利でした。

### 3️⃣ 自動取り込みスクリプトを配置

完成形は「USBで挿したら勝手に回る」ことを優先し、
レコーダーの `RECORD/` 配下を検出して取り込み → WhisperX実行 → 出力保存までをスクリプトにまとめました。

- `/Volumes/*/RECORD` を探索して録音ファイルを取り込み
- `~/whisperx_test/audio/<日付>/` に日付別で整理（ファイル名が `YYYY-MM-DD` ならその日付、そうでなければ `日付不明`）
- WhisperXで `output_format=all`（json/txt/srtなど）を出力
- 既に出力がある場合はスキップ
- 実行ログを `~/Library/Logs/zd46/pipeline.log` に残す

最低限、どんな処理をしているか分かるように、`zd46_pipeline.py` の要点だけ抜粋します。
（そのまま動く完全版ではなく、記事用の要約コードです）

```python
# zd46_pipeline.py（要点のみ抜粋）
import glob
import logging
import re
import shutil
import subprocess
from pathlib import Path

AUDIO_DST = Path.home() / "whisperx_test" / "audio"
OUT_DST   = Path.home() / "whisperx_test" / "outputs"
LOG_PATH  = Path.home() / "Library" / "Logs" / "zd46" / "pipeline.log"
DATE_RE   = re.compile(r"^(\d{4}-\d{2}-\d{2})")


def find_record_root() -> Path | None:
    # レコーダーがマウントされる想定: /Volumes/<DEVICE>/RECORD
    hits = glob.glob("/Volumes/*/RECORD")
    return Path(hits[0]) if hits else None


def classify_date(filename: str) -> str:
    # ファイル名が YYYY-MM-DD で始まるなら日付で分類
    m = DATE_RE.match(Path(filename).name)
    return m.group(1) if m else "日付不明"


def run_whisperx(infile: Path, out_dir: Path, initial_prompt: str | None = None) -> None:
    # venv の python を使って whisperx を実行（model/vad/output_format がポイント）
    py = Path.home() / "venvs" / "whisperx" / "bin" / "python"
    cmd = [
        str(py), "-m", "whisperx", str(infile),
        "--model", "medium",
        "--language", "ja",
        "--beam_size", "1",
        "--compute_type", "int8",
        "--vad_method", "silero",
        "--output_format", "all",
        "--output_dir", str(out_dir),
    ]
    if initial_prompt:
        cmd += ["--initial_prompt", initial_prompt]
    subprocess.run(cmd, check=True)


def main() -> None:
    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(filename=str(LOG_PATH), level=logging.INFO)

    record_root = find_record_root()
    if not record_root:
        logging.info("RECORD not found; skip")
        return

    # 例: RECORD 配下の音声を日付別に移動してから文字起こし
    for src in sorted(record_root.glob("**/*.m4a")):
        day = classify_date(src.name)
        dst_dir = AUDIO_DST / day
        dst_dir.mkdir(parents=True, exist_ok=True)
        dst = dst_dir / src.name
        shutil.move(str(src), str(dst))

        out_dir = OUT_DST / day
        out_dir.mkdir(parents=True, exist_ok=True)

        # 既に .txt がある場合はスキップ（重いので二重実行を避ける）
        if (out_dir / f"{dst.stem}.txt").exists():
            logging.info("skip (already exists): %s", dst.name)
            continue

        run_whisperx(dst, out_dir)


if __name__ == "__main__":
    main()
```

このように「USBで挿す → RECORD を探す → 日付別に取り込み → WhisperXで `json/txt/srt` を出す」という部分を最小構成にしています。
（完全版では、対象拡張子の追加、0件時スキップ、アンマウント、verboseログなども入れています）

### 4️⃣ Automator / スクリプトエディタから自動実行

私は「レコーダーを挿す → 自動でスクリプトが走る」運用にしています。
Automator側は **1行**でOKです。

```bash
% /usr/bin/python3 "$HOME/zd46_pipeline.py"
```

---

## （参考）話者分離（Pyannote.audio）に挑戦したときの手順

WhisperXは `--diarize` を付けると話者分離まで一気通貫でやれます。
そこで当初は Pyannote.audio も入れて、話者分離付きの文字起こしを試しました。

### Pyannoteのインストール

```bash
% pip install "pyannote.audio>=3.1,<4.0"
```

### Hugging Face トークン（pyannote利用に必要）

Pyannoteの一部モデルを使うために、Hugging Face の利用規約に同意し、トークンを発行します。

```bash
% export HF_TOKEN="hf_xxxxxxxxxxxxx"
% echo 'export HF_TOKEN="hf_xxxxxxxxxxxxx"' >> ~/.zprofile
```

### 話者分離つきで実行（当時のコマンド例）

```bash
% export CT2_USE_METAL=1
% whisperx ~/whisperx_test/audio/sample.m4a \
%   --model large-v3 \
%   --language ja \
%   --initial_prompt "妻との会話です。朝の準備やお出かけの話をしています。" \
%   --output_dir ~/whisperx_test/outputs \
%   --compute_type int8 \
%   --diarize --vad_method pyannote \
%   --hf_token "$HF_TOKEN"
```

### うまくいかなかった点（体感）

やってみた結果、**文字起こし自体はそれなりに成立**する一方で、
話者分離は「実用には厳しい」ことが多かったです。

- 話者ラベルが頻繁に入れ替わる（同じ人が別のSPEAKERになる）
- 短い会話だとサンプルが足りず、推定が不安定になりやすい
- 早口／被り（同時発話）／生活音（家事・子どもの声）があるとさらに崩れる
- 方言＋テンポ速めの会話だと、VAD（区間検出）がずれて連鎖的に崩れる

「話者分離まで完璧にやる」ほど、録音条件（マイク位置、部屋の反響、話し方、会話の長さ）に依存してしまい、
家庭利用では運用コストが上がると判断しました。

### じゃあどうしたか：話者分離を捨てて“要約に寄せた”

ここで方針転換しました。

- 音声→テキスト（WhisperX）はローカルで確実に出す
- 「誰が言ったか」の厳密さは捨てる
- 代わりに **ChatGPTの推論で会話の流れと要点を再構成**してもらう

この構成だと、文字起こしが多少ラフでも、
「今日何を話したか／決めたか／次に何をするか」が十分に取り出せます。

## ディレクトリ構成
```bash
~/zd46_pipeline.py   ← 自動取り込み＆文字起こしスクリプト
~/whisperx_test/
 ├── audio/          ← 取り込んだ録音ファイル（YYYY-MM-DDで日付別に整理）
 ├── outputs/        ← WhisperXの出力（txt, srt, json など。日付別）

~/Library/Logs/zd46/pipeline.log  ← 実行ログ
```

レコーダーの録音が溜まっていても、USBで挿したタイミングで一気に取り込み・処理できるので、  
「1日分録る → PCに挿す → 少し待つ → 要約する」という運用に落とし込めました。

## 🚀 実行（完成形）

完成形は、基本的に **挿したら走る** なので、普段はコマンドを叩きません。  
ただ、動作確認したいとき用に書いておきます。

### 手動で実行（デバッグ）

```bash
% /usr/bin/python3 ~/zd46_pipeline.py --verbose
```

（必要なら）会話の文脈を補助するプロンプトも渡せます。

```bash
% /usr/bin/python3 ~/zd46_pipeline.py \
%   --initial-prompt "妻との会話です。朝の準備やお出かけの話をしています。"
```

### （参考）話者分離ありで試したときのコマンド

```bash
% export CT2_USE_METAL=1
% whisperx ~/whisperx_test/audio/sample.m4a \
%   --model large-v3 \
%   --language ja \
%   --initial_prompt "妻との会話です。子どもの名前はxxx、私はxxx、妻はxxxです。朝の準備やお出かけの話をしています。" \
%   --output_dir ~/whisperx_test/outputs \
%   --compute_type int8 \
%   --diarize --vad_method pyannote \
%   --hf_token "$HF_TOKEN"
```

## ✅ 完成形（話者分離を捨てて運用できるようになった）

### この構成で運用できるようになった理由

話者分離を頑張るよりも、最終的にはこの構成のほうが“毎日使える”状態になりました。

- **行動が短い**：1日録る → USBで挿す → 少し待つ → jsonを貼る、で終わる
- **要約の強さ**：書き起こしが多少雑でも、会話の筋や決定事項は推論で補完されやすい
- **コストが増えない**：API課金なしで、ChatGPTの月額範囲で運用できる
- **調整地獄を避けられる**：話者分離は録音条件に依存しがちで、家庭利用だと継続コストが高い

狙いは「完璧な文字起こし」ではなく、
**“今日なに話したか”を後から取り出せること**だったので、
要約中心の設計に寄せるのが正解でした。

## 結果（話者分離は崩れた。でも要約は成立した）

まず前提として、以下の書き起こしは **話者分離に挑戦していた時（v1）** の出力例です。  
SPEAKERが分かれているように見えますが、実際にはラベルが安定していません。

### v1：話者分離付きの生データ例（うまく分かれていない）

```text
[SPEAKER_02]: 明日の準備はこれで終わり。これ、今何の準備?明日、朝時間内からxxx（子どもの名前）の洗濯をする準備。じゃあ、明日とりあえず何個サロンつくやん。その後に、
[SPEAKER_02]: 化粧をして、行きながら行く途中でドライブスルーとかコンビニに行って、ご飯を買って、大人はそれを食べながら行って、美味しい後援についてはxxx（子どもの名前）のご飯。ついてかな?出る前。出てる最中でもいいかなって思ってた。出てるんすか?ああ、それでもいいな。xxx（私の名前）運転してる後ろで壊せるの。
[SPEAKER_02]: まあ、やってみて。一旦。最悪の場合は、時間がないとなったらそうする。化粧している間は、ベビーカーを乗せておくって感じだよ。その時に、飯を食わせれそうだったら、そこで食わせるとかもありやけどな。時間によるな。それに合わせて朝ごはん食べさすからさ、もうほんじゃ朝ごはん早めにした方がいい。
[SPEAKER_04]: 4、5時間開けば、昼ごはんちゃんと食べてくれると思う。どっちがいいだろう?メイクが10時から何時?メイクは、11時半だったかな。
[SPEAKER_04]: メイク中のご飯は早すぎる?早くつく可能性もあるけど、メイク1時間半あるとは限らないし。
[SPEAKER_02]: もう少しポーズを見ておかないと、1時間くらいしか写真撮影の時間がないから、もう少し見ておこう。
```

自身の会話であっても、書き起こしされた生テキストは読みにくく、内容を思い出しにくい。ここだけ見ると「話者分離は失敗」に見えます。  
でも、**このテキストとともに出力されたJSONをそのままChatGPTに貼って要約させたら、ちゃんと“議事録っぽく”まとまった**のが今回の収穫でした。

---

### v2：ChatGPTで要約した例（推論で会話の筋が通る）

※下は上のテキストから推論して作った要約例です（聞き取れない箇所は「推測」扱い）。

#### 要約

- 明日の外出（撮影？）に向けて、朝の段取りを相談している
- 子どもの食事・移動（ベビーカー等）を「いつ／どこで」やるかが主題
- 大人の食事は移動中にドライブスルー／コンビニで買って済ませる案
- 時間が押した場合の代替案も確認（化粧中に子ども対応をどうするか等）
- メイクの時間は **11:30頃**の想定で、撮影時間が短い（約1時間）ので事前にポーズ確認が必要

#### 決定事項／ToDo（読み取れる範囲）

- 子どもの食事タイミングは「昼ごはんをちゃんと食べられる間隔（4〜5時間）」を目安に調整する
- 朝ごはんは早めにする可能性がある
- ベビーカー等の準備をしておく（化粧中の運用も想定）
- 撮影が短いので、事前にポーズを少し見ておく

#### 論点の推移

1. 明日の準備
2. 移動中・到着前後の食事プラン（大人／子ども）
3. 時間がない場合のリカバリ案
4. メイク開始時刻と所要、撮影時間の制約
5. 事前準備（ポーズ確認）

#### 不確かな点（推測）

- 「サロン」は誤認識の可能性が高く、実際は別の単語かもしれない
- 「後援」は「公園」などの可能性がある（文脈上、外出先の話に見える）

---

不完全な点はありますが、要約が強いので「こういう話をしたんだな」がちゃんと取り出せます。  
**話者分離の精度を追うよりも、要約に寄せた方が目的達成に近かった**という結論になりました。

そしていまは v2（話者分離なし）に落としたことで、  
レコーダーを挿したら処理が走り、出力を貼るだけで要約が取れる運用が回っています。

## まとめ

当初は「Plaud Note Proっぽいものを全部ローカルで再現したい」と考えて始めました。  
具体的には、

- ローカルで録音を取り込み
- ローカルで文字起こし
- できれば話者分離まで
- それを見返して“第二の記憶”にする

という欲張り構成です。

でも実際に触ってみると、話者分離はかなり“環境依存の機能”でした。  
家庭内の音声は、

- 生活音が混ざる
- 子どもの声が被る
- 会話が短く途切れがち
- 方言や早口もある

みたいな条件が普通に起きます。

そこで発想を変えて、  
**「音声→テキスト（ローカル）は確実にやる。読みやすさは要約に任せる」**に寄せました。

- ローカル文字起こし（WhisperX）でコストを確保
- USBに挿すだけで回る自動化（`zd46_pipeline.py` + Automator）で運用を軽くする
- 最後の整形はChatGPTの推論能力に寄せて、全体の体験品質を上げる

この構成だと、  
「今日なに話した？」を後から取り出す体験が、実際に毎日の運用として回るようになりました。

※注意：要約のためにChatGPTへテキストを貼り付ける以上、そのテキストはChatGPT側に送信されます。  
扱う内容（家庭の会話、仕事の会話など）によっては、  
貼り付ける範囲を絞る／固有名詞を伏せる／要約をローカルLLMに寄せる、などの工夫が必要です。

「全部を完璧に自動化」よりも、  
**“面倒が増えない形で、ちゃんと使える”**に落としたのが一番の学びでした。

そして会話ログが日々積み上がるほど、「過去の自分が何を考え、何を決めたか」を後から参照できるようになります。  
冒頭で書いた「第二の私」を地道に育てていくことができそうでワクワクしています。
